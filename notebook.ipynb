{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ec8d6a-4218-49b9-a549-0977cec82967",
   "metadata": {
    "id": "MVpsYfWg3z0B"
   },
   "source": [
    "# PW1 - Handwritten character recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d682452d-e56e-4de8-a420-4418d63790a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your name here (e.g. \"Edmond Dant√®s\") so I can grade your work\n",
    "your_name = \"Paul-Constant TOUDRET\"\n",
    "assert your_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f8538cf7-81e0-481a-8059-be38b611aafb",
   "metadata": {
    "id": "8CcAqNjJ3z0F"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math, sys, os, torch, torchvision\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c864b86e-6e6b-4d8a-82c8-6822072a676f",
   "metadata": {
    "id": "3Wxb9pdV3z0F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: False \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa038da6-b812-4053-9b72-8ff97ceb3b9b",
   "metadata": {
    "id": "1Sjq8zzf3z0G"
   },
   "source": [
    "We will be training many models. Select a number of epochs to train each model. If you are using a slow machine, or if you want to restart training often and have many development iterations, we suggest `NUM_EPOCH = 2`. If you are using a fast machine, or have a GPU available, of if you are confident that you can write accurate code first try, you will get better accuracies by increasing this constant. You could be able to afford up to `NUM_EPOCH = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "137adce6-13cc-4ccc-aaf3-42230e322a9b",
   "metadata": {
    "id": "L9CF0H4O3z0G"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce850e9-ef4e-4323-af2d-be6b94e98994",
   "metadata": {
    "id": "65e20f5e"
   },
   "source": [
    "# Part A - Linear, MLP, and CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aff4bd-8b1d-4531-89d8-64ce4e2b6357",
   "metadata": {
    "id": "KSAiV2ov3z0H"
   },
   "source": [
    "## Handwritten digit recognition dataset\n",
    "\n",
    "We will use the MNIST database (Modified National Institute of Standards and Technology database). It contains tens of thousands of pictures of handwritten digits. This database was compiled in 1994, as part of the effort in the 1990s to standardize automation of sorting devices with human input, for instance sorting mail with handwritten postal codes at the post office. This is now often considered one of the first real successes of neural networks, and the first easy example on which performance of new such algorithms is tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429898a-0f57-4cdf-9aa6-7b1121ee4e53",
   "metadata": {},
   "source": [
    "Load the dataset (train and test splits) using `torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "59e1c523-4b78-492a-b103-2861af8c3d89",
   "metadata": {
    "id": "Zu3hU4dQ3z0H"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "root_dir = './data/MNIST/'\n",
    "\n",
    "\"\"\"\n",
    "Code taken from Torch Doc\n",
    "\"\"\"\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=root_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=root_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fbaba-b01f-46d0-a1d7-47c59faaf042",
   "metadata": {},
   "source": [
    "How many examples in each split? \n",
    "\n",
    "Plot the first image and label of the training set using `matplotlib`\n",
    "\n",
    "What is the input dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "083f07f4-e129-4e9b-b26e-a32fd4bdb99d",
   "metadata": {
    "id": "9fgMls5P3z0I",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training example : 60000\n",
      "number of test example : 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Number of examples\n",
    "\n",
    "print(f\"number of training example : {len(training_data)}\")\n",
    "print(f\"number of test example : {len(test_data)}\")\n",
    "\n",
    "### First image and label of training\n",
    "\"\"\"\n",
    "Code taken from Torch Doc\n",
    "\"\"\"\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 1, 1\n",
    "for i in range(1, cols * rows + 1):\n",
    "    #sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[0]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc2fbd26-b017-435a-822e-ad382eabd8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of image : 1\n",
      "height of image : 28 pixels\n",
      "width of image : 28 pixels\n"
     ]
    }
   ],
   "source": [
    "### Dimension of the input\n",
    "\n",
    "d, w, h = training_data[0][0].shape\n",
    "print(f\"dimension of image : {d}\")      # Only Grey Scale\n",
    "print(f\"height of image : {h} pixels\")\n",
    "print(f\"width of image : {w} pixels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6380c03-1427-4dea-974e-5a38621ea6c1",
   "metadata": {
    "id": "Guv5_hY63z0L"
   },
   "source": [
    "# A.1 - Linear features\n",
    "\n",
    "We start with a very simple model, linear with respect to pixel values.\n",
    "Use a `preprocess` function to downsample the image to 7x7 pixels, then flatten it and use a `torch.nn.Linear` model.\n",
    "\n",
    "The torch average-pooling function is `torch.nn.functional.avg_pool2d`, check the documentation to set the arguments properly.\n",
    "DO NOT use your implementation of average-pooling, it would take prohibitively long to train and you would not finish the practical.\n",
    "If the training takes too long, go back to the first section and lower the `NUM_EPOCH` constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb8e5d-7d84-4a59-aa5d-e942ffa22aa8",
   "metadata": {},
   "source": [
    "Again, use matplotlib to visualize an example of downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ac111d5c-dfa2-44e2-9a57-f2f4c2caff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of image : 1\n",
      "height of image : 28 pixels\n",
      "width of image : 28 pixels\n",
      "dimension of flatten image : 1\n",
      "width of flatten image : 49 pixels\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "preprocess = nn.Sequential(\n",
    "    nn.AvgPool2d(\n",
    "    kernel_size=3,\n",
    "    stride=4\n",
    "    ),\n",
    "    nn.Flatten()\n",
    ")\n",
    "\"\"\"\n",
    "Stride = 4 -> we divide the dimensions by 4 to get 7 X 7 pixels\n",
    "Kernel size is here arbitratry\n",
    "\"\"\"\n",
    "f_img = preprocess(training_data[0][0])\n",
    "\n",
    "d, w, h = img.shape\n",
    "print(f\"dimension of image : {d}\")      # Only Grey Scale\n",
    "print(f\"height of image : {h} pixels\")\n",
    "print(f\"width of image : {w} pixels\")\n",
    "\n",
    "d, w = f_img.shape\n",
    "print(f\"dimension of flatten image : {d}\")      # Only Grey Scale\n",
    "print(f\"width of flatten image : {w} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6aa4adcb-be17-4f0d-ba9f-4da062227230",
   "metadata": {
    "id": "2v3GqEPU3z0L"
   },
   "outputs": [],
   "source": [
    "### YOUR ( MODEL / PREPROCESSING ) CODE HERE ###\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            \n",
    "            # Second Step -> Apply only Linear layer\n",
    "            self.classifier = nn.Linear(self.input_size, self.output_size)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.input_size)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5becae4-e27f-43ed-a1c5-ced8c16f4cac",
   "metadata": {
    "id": "qeWZ7DeNMG20"
   },
   "source": [
    "## A.2 - Loss and optimizer\n",
    "Create a cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2cb4827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "from tqdm import tqdm # only for print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a85b789e",
   "metadata": {
    "id": "a85b789e"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "nn1 = LinearModel(\n",
    "    input_size=49,\n",
    "    output_size=10\n",
    ")\n",
    "\n",
    "c_loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nn1.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aec31b-3d2b-4b29-ad85-e88a6e25660a",
   "metadata": {
    "id": "ZCnlsh9iMhx_"
   },
   "source": [
    "## A.3 - Training and testing loops\n",
    "Finally, create the functions `train(model, epoch, preprocess, optimizer)` and `test(model)` to train (one epoch with SGD and a learning rate of $10^{-3}$) and test your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0b820979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing img\n",
      "preprocess en cours : 60000\r"
     ]
    }
   ],
   "source": [
    "preprocess_train = []\n",
    "print(f'preprocessing img')\n",
    "for img, label in training_data:\n",
    "    preprocess_train.append((preprocess(img), label))\n",
    "\n",
    "    if len(preprocess_train) % 1000 == 0:\n",
    "        print(f\"preprocess en cours : {len(preprocess_train)}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "521e5485-b676-4f73-bedc-d35c5fcfd394",
   "metadata": {
    "id": "iMXijrch3z0L"
   },
   "outputs": [],
   "source": [
    "def train(model : nn.Module, epoch, preprocess, optimizer):\n",
    "    ### YOUR CODE HERE ###\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    n_train = len(preprocess_train[:])\n",
    "\n",
    "    for e in range(epoch):\n",
    "        print(f'epoch n : {e + 1} / {epoch} ')\n",
    "\n",
    "        for input, target in preprocess_train:\n",
    "            target = torch.tensor(target, dtype=torch.int8)\n",
    "            optimizer.zero_grad()\n",
    "            ouput = model.forward(input)\n",
    "\n",
    "            loss = c_loss(ouput, target)\n",
    "            accuracy += loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'for epoch : {e} --> accuracy : {accuracy} / {len(input)}')\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    for input, target in preprocess_train:\n",
    "        f_acc = c_loss(input, model(input))\n",
    "\n",
    "    optimizer, train_accuracy = optimizer, f_acc/n_train\n",
    "    return optimizer, train_accuracy\n",
    "\n",
    "def test(model, preprocess):\n",
    "    ### YOUR CODE HERE ###\n",
    "    accuracy = None\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa359382-2580-44ae-bcdb-3f18cf1f4c61",
   "metadata": {
    "id": "_t4SiXk33z0L"
   },
   "source": [
    "You should get at least 85\\% test accuracy even with only 2 epochs. We will be aiming for around 95\\% test accuracy and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6f12bf85-45c6-41d9-87f3-0601bc4b6339",
   "metadata": {
    "id": "nBmfvtl6UbUe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n : 1 / 2 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### YOUR CODE HERE ###\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, epoch, preprocess, optimizer)\u001b[39m\n\u001b[32m     13\u001b[39m optimizer.zero_grad()\n\u001b[32m     14\u001b[39m ouput = model.forward(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m loss = \u001b[43mc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m accuracy += loss\n\u001b[32m     19\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:1295\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Expected input batch_size (1) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "train(\n",
    "    model=nn1,\n",
    "    epoch=NUM_EPOCH,\n",
    "    preprocess=preprocess_train,\n",
    "    optimizer=optimizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28bb4e-7178-4340-9247-f69591e86dcf",
   "metadata": {
    "id": "RrwYAMMBEUPN"
   },
   "source": [
    "## A.4 - Multi-layer perceptron (MLP)\n",
    "\n",
    "Create a class MLP that creates an MLP of given width and depth, and use it to create a 3-layer MLP of width $100$. We will assume that `width > 0` and `depth > 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79cb6479-d475-4f0e-b0f7-fdd107d9e835",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8353cd9",
    "outputId": "c7f5eeaf-0638-45c3-842e-4372d21ff712"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca5a11-93e4-47ab-81f1-e4ee3cae45c9",
   "metadata": {
    "id": "v1czyC9R3z0R"
   },
   "source": [
    "# A.5 - Deep convolutional model\n",
    "\n",
    "Write a convolutional model, with learned features.\n",
    "Use two layers, one convolutional with 8 filters of size 3x3, then take a relu and max-pool with kernel size 2, and finally flatten and add a Linear layer. You can use the identity as pre-processing function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4933e318-5521-4bf8-b011-bdb95ed0d8b0",
   "metadata": {},
   "source": [
    "\n",
    "Here is a little animation to remind you of the sliding window principle of convolutions.\n",
    "\n",
    "![conv](https://github.com//vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44c87b30-36d7-4567-b009-a41413afac5b",
   "metadata": {
    "id": "l4QOi_oe3z0R"
   },
   "outputs": [],
   "source": [
    "class ConvModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvModel, self).__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    def forward(self,x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0313eed-03ad-461d-9ddb-bb690c631149",
   "metadata": {
    "id": "pbQ1LUqf3z0R"
   },
   "source": [
    "You should be able to get around 97\\% to 98\\% accuracy with this model. Try increasing the NUM_EPOCH constant and watch what happens to test accuracy and train accuracy as training progresses further.\n",
    "\n",
    "Write a deeper convolutional model, with one convolutional layer as previously, but three linear layers with relu activations after that.\n",
    "Use `h = 100` hidden neurons. How does the test accuracy compare with the previous two-layer network ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "792791e1-0514-4749-a3fb-d27eb10a1bd2",
   "metadata": {
    "id": "YM-OhC123z0R"
   },
   "outputs": [],
   "source": [
    "class ConvDeepModel(torch.nn.Module):\n",
    "    def __init__(self, h=100):\n",
    "        super(ConvDeepModel, self).__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    def forward(self,x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd763b-b36a-461c-ad34-f472e62ce46d",
   "metadata": {},
   "source": [
    "## A.6 Visualisations of convolutions\n",
    "\n",
    "After training your model, let's see what features it has learned!\n",
    "\n",
    "Plot an image from the test set then plot all 8 feature maps extracted by the convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e5284c2-4210-4ccc-bf05-5e722bdabf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc457ad1-3240-4ffb-a8f3-98c6554dfce5",
   "metadata": {
    "id": "riu_K1at3z0R"
   },
   "source": [
    "# Part B - Residual models\n",
    "\n",
    "## B.1 - Residual blocks\n",
    "\n",
    "Write a residual block with two linear layers to learn a function $\\mathbb{R}^d \\to \\mathbb{R}^d$ with $h < d$ hidden neurons.\n",
    "Write a convolutional residual block with the same idea. What hyperparameter acts as the number of hidden neurons in convolutional blocks ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec298d4-4372-4a1f-be53-a278d051c0f6",
   "metadata": {
    "id": "gkCILVwd3z0R"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, d, h):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    def forward(self,x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc0097d-269c-40f4-b182-bdc79bd5c512",
   "metadata": {
    "id": "huKWOvVc3z0S"
   },
   "source": [
    "## B.2 - Stacking residual blocks\n",
    "\n",
    "Use a single convolution layer, followed by a relu and max-pool, then an arbitrary number of residual blocks as defined above, and finish with a linear layer. Can you match the accuracy of the two-layer network ? Can you exceed it ? What happens when you increase the number of layers ? Look at the details of the ResNet architecture on the lecture's slides to get an idea of how to increase the number of hidden neurons and the number of layers. One of the strengths of ResNets was there relatively low number of parameters compared\n",
    "to a multi-layer architecture like that of the previous section, does this show in your experiments ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d4de4-11e8-490c-8262-064b3e841fa5",
   "metadata": {
    "id": "U4gg9uWg3z0S"
   },
   "outputs": [],
   "source": [
    "class ResidualModel(torch.nn.Module):\n",
    "    def __init__(self, l, h, k=3, out=8):\n",
    "        super(ResidualModel, self).__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    def forward(self,x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df418703-b1e1-4276-8ba7-dd5622c7b7cd",
   "metadata": {
    "id": "2nes_ZtBoBu0"
   },
   "source": [
    "# Part C - Reimplementing loss functions\n",
    "\n",
    "## C.0 - Combining losses\n",
    "First, we recall that, for a batch of score vectors $s\\in\\mathbb{R}^{n\\times C}$ and true labels $y\\in[1,C]^n$, **cross entropy** is defined as\n",
    "$$CE(s, y) = -\\frac{1}{n}\\sum_{i=1}^n \\log\\left( \\mbox{softmax}(s_i)_{y_i} \\right)$$\n",
    "\n",
    "where $\\mbox{softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$ is the probability associated to class $i\\in[1,C]$ for a score vector $x\\in\\mathbb{R}^C$.\n",
    "\n",
    "Let's try to compute cross-entropy in three different ways (see the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)):\n",
    "1. Using `nn.CrossEntropyLoss()`.\n",
    "2. Using `nn.NLLLoss()` and `nn.LogSoftmax()`.\n",
    "3. Using `nn.NLLLoss()` and `nn.Softmax()`.\n",
    "\n",
    "Check that the output is the same for all three methods on Gaussian random scores `torch.randn(n_batch, n_classes)` and random labels `torch.randint(0, n_classes, [n_batch])`, where `n_batch=4` and `n_classes=10`. Note that the scores are real valued vectors while the labels are integers corresponding to the true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1b6c172",
   "metadata": {
    "id": "e1b6c172"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba900c6-f7b2-4179-a2f1-f1eede4b6697",
   "metadata": {
    "id": "TWKaTBVd5ftN"
   },
   "source": [
    "## C.1 - Re-implementation\n",
    "Now re-implement cross-entropy using base functions (`torch.log`, `torch.exp`, `torch.sum`, etc...). Verify that your function returns the same value as Pytorch's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "EfA-3-E7qwgF",
   "metadata": {
    "id": "EfA-3-E7qwgF"
   },
   "outputs": [],
   "source": [
    "def ce(logits, targets):\n",
    "    ### YOUR CODE HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ec13a-330a-4073-99b9-2eb9dd42d1f8",
   "metadata": {
    "id": "OFG0QfKN7WtO"
   },
   "source": [
    "## C.2 - Stability analysis\n",
    "Softmax probabilities can be relatively unstable due to their use of exponentials. Pytorch implementations thus usually use log probas or logits to avoid overflows or floating point errors. Test all methods (including your own) on Gaussian random scores of standard deviation equal to $100$. Which methods are stable? Why? Is it an issue in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "741771f5-864d-446a-b654-3b4f5a2598ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecaaf7c-b21f-4e18-ac53-1a77adbc60bf",
   "metadata": {
    "id": "Y3y4BfwbBIGy"
   },
   "source": [
    "Re-implement a stable version of cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83d00dab-a37e-48da-880f-81f10efdc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_ce(logits, targets):\n",
    "    ### YOUR CODE HERE ###\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
